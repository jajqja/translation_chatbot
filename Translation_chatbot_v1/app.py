import streamlit as st
import json
from typing import Dict, List
import pandas as pd
import fitz 

# Import your modules
from chatbot import Chatbot
from chunkprocess import (
    split_chunk_text,
    build_translation_prompt,
    parse_llm_json_output,
    merge_keywords,
    get_example_final_sentences
)

# Page configuration
st.set_page_config(
    page_title="AI Translation Assistant",
    page_icon="üåê",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .chunk-container {
        background-color: #f9f9fb; /* N·ªÅn tr·∫Øng */
        color: #1e293b;            /* Ch·ªØ ƒë·∫≠m d·ªÖ ƒë·ªçc */
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
    }
    .translation-result {
        background-color: #e6f4ea; /* N·ªÅn xanh nh·∫°t */
        color: #1a3c34;            /* Ch·ªØ xanh ƒë·∫≠m */
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 4px solid #28a745;
    }
    .keywords-box {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 4px solid #ffc107;
    }
</style>
""", unsafe_allow_html=True)

def init_session_state():
    """Initialize session state variables"""
    if 'global_keywords' not in st.session_state:
        st.session_state.global_keywords = {}
    if 'translation_history' not in st.session_state:
        st.session_state.translation_history = []
    if 'current_translation' not in st.session_state:
        st.session_state.current_translation = None

def display_keywords(keywords: Dict, title: str = "Keywords"):
    """Display keywords in a formatted way"""
    if keywords:
        st.markdown(f"**{title}:**")
        cols = st.columns(3)
        for i, (en, vi) in enumerate(keywords.items()):
            with cols[i % 3]:
                st.markdown(f"‚Ä¢ `{en}`: {vi}")

# Kh·ªüi t·∫°o chatbot 1 l·∫ßn
@st.cache_resource
def load_chatbot():
    return Chatbot()

chatbot = load_chatbot()

def main():
    init_session_state()
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>üåê AI Translation Assistant</h1>
        <p>Intelligent English to Vietnamese Translation with Context & Terminology Management</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar configuration
    with st.sidebar:
        st.header("Configuration")
        
        # Translation settings
        st.subheader("Translation Settings")
        
        domain = st.selectbox(
            "Domain",
            ["general", "technical", "medical", "legal", "business", "academic"],
            index=0
        )
        
        style = st.selectbox(
            "Style",
            ["formal", "casual", "academic"],
            index=0
        )
        
        chunk_size = st.slider(
            "Chunk Size (characters)",
            min_value=800,
            max_value=1000,
            value=800,
            step=50
        )
        
        
        # Global keywords management
        st.subheader("Danh s√°ch Keywords")
        if st.session_state.global_keywords:
            st.write(f"T·ªïng s·ªë keywords: {len(st.session_state.global_keywords)}")
            if st.button("Clear Keywords"):
                st.session_state.global_keywords = {}
                st.success("Keywords cleared!")
        
        # Export keywords
        if st.session_state.global_keywords:
            keywords_json = json.dumps(st.session_state.global_keywords, indent=2, ensure_ascii=False)
            st.download_button(
                label="T·∫£i v·ªÅ Keywords",
                data=keywords_json,
                file_name="translation_keywords.json",
                mime="application/json"
            )
    
    # Main content area
    st.header("Nh·∫≠p vƒÉn b·∫£n ho·∫∑c t·∫£i l√™n file")

    # Text input methods
    input_method = st.radio(
        "Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu",
        ["Nh·∫≠p tr·ª±c ti·∫øp", "T·∫£i file t·ª´ m√°y"],
        horizontal=True
    )
    
    input_text = ""

    if input_method == "Nh·∫≠p tr·ª±c ti·∫øp":
        input_text = st.text_area(
            "Nh·∫≠p vƒÉn b·∫£n c·∫ßn d·ªãch:",
            height=300,
            placeholder="Paste your English text here..."
        )
    else:
        uploaded_file = st.file_uploader(
            "T·∫£i file vƒÉn b·∫£n (.txt, .md, .pdf):",
            type=['txt', 'md', 'pdf']
        )

        input_text = ""

        if uploaded_file:
            file_type = uploaded_file.name.split(".")[-1].lower()

            if file_type in ["txt", "md"]:
                input_text = str(uploaded_file.read(), "utf-8")

            elif file_type == "pdf":
                pdf_doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                text_chunks = [page.get_text() for page in pdf_doc]
                input_text = "\n\n".join(text_chunks)
    
    # Manual keywords input
    st.subheader("Nh·∫≠p Keywords th·ªß c√¥ng (Optional)")
    manual_keywords = st.text_area(
        "Nh·∫≠p keywords (ƒë·ªãnh d·∫°ng JSON):",
        placeholder='{"machine learning": "h·ªçc m√°y", "algorithm": "thu·∫≠t to√°n"}',
        height=100
    )
    
    if manual_keywords:
        try:
            manual_kw_dict = json.loads(manual_keywords)
            st.session_state.global_keywords.update(manual_kw_dict)
            st.success(f"Added {len(manual_kw_dict)} keywords")
        except json.JSONDecodeError:
            st.error("Kh√¥ng th·ªÉ ph√¢n t√≠ch c√∫ ph√°p JSON. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng.")

    st.subheader("D·ªãch vƒÉn b·∫£n")

    if st.button("D·ªãch vƒÉn b·∫£n", type="primary", disabled=not input_text):
        if not input_text:
            st.warning("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n c·∫ßn d·ªãch")
        else:
            translate_text(input_text, domain, style, chunk_size)
    
    # Display current translation
    if st.session_state.current_translation:
        st.divider()
        display_translation_results(st.session_state.current_translation)
    
    # Translation history
    if st.session_state.translation_history:
        st.header("üìö L·ªãch s·ª≠")
        
        with st.expander("Xem l·ªãch s·ª≠ d·ªãch thu·∫≠t", expanded=True):
            for i, translation in enumerate(reversed(st.session_state.translation_history)):
                st.markdown(f"**D·ªãch thu·∫≠t {len(st.session_state.translation_history) - i}**")
                st.write(f"**Chunks:** {translation['chunks']}")
                st.write(f"**T·ªïng s·ªë Keywords:** {len(translation['global_keywords'])}")
                st.markdown("---")

def translate_text(text: str, domain: str, style: str, chunk_size: int):
    """Main translation function"""
    try:
        # Split text into chunks
        chunks = split_chunk_text(text, chunk_size)
        
        # Progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        results = {
            "chunks": len(chunks),
            "translations": [],
            "global_keywords": st.session_state.global_keywords.copy()
        }
        
        translated_context = ""
        
        for i, chunk in enumerate(chunks):
            status_text.text(f"D·ªãch chunk {i+1}/{len(chunks)}...")
            
            # Build prompt
            prompt = build_translation_prompt(
                chunk, 
                translated_context, 
                results["global_keywords"],
                domain,
                style
            )
            
            # Get translation from LLM
            llm_response = chatbot.generate(prompt)
            translation, keywords = parse_llm_json_output(llm_response)
            
            # Merge keywords
            results["global_keywords"] = merge_keywords(
                results["global_keywords"], 
                keywords
            )
            
            # Update context for next chunk
            translated_context = get_example_final_sentences(translation)
            
            results["translations"].append({
                "chunk_index": i,
                "original": chunk,
                "translation": translation,
                "keywords": keywords,
                "prompt": prompt
            })
            
            progress_bar.progress((i + 1) / len(chunks))
        
        # Update global keywords
        st.session_state.global_keywords = results["global_keywords"]
        
        # Store results
        st.session_state.current_translation = results
        st.session_state.translation_history.append(results)
        
        status_text.text("ƒê√£ d·ªãch th√†nh c√¥ng!")
        progress_bar.empty()
        
    except Exception as e:
        st.error(f"L·ªói d·ªãch: {str(e)}")

def display_translation_results(results: Dict):
    """Display translation results"""
    st.subheader("K·∫øt qu·∫£ D·ªãch Thu·∫≠t")
    
    # Summary
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Chunks", results["chunks"])
    with col2:
        st.metric("Total Keywords", len(results["global_keywords"]))
    
    # Tabs for different views
    tab1, tab2, tab3 = st.tabs(["B·∫£n d·ªãch", "Theo t·ª´ng ƒëo·∫°n", "T·ª´ kh√≥a"])
    
    with tab1:
        st.markdown("### B·∫£n d·ªãch ho√†n ch·ªânh")
        full_translation = "".join([t["translation"] for t in results["translations"]])
        st.markdown(f'<div class="translation-result">{full_translation}</div>', unsafe_allow_html=True)
        
        # Download option
        st.download_button(
            label="T·∫£i xu·ªëng B·∫£n d·ªãch",
            data=full_translation,
            file_name="translation.txt",
            mime="text/plain"
        )
    
    with tab2:
        st.markdown("### Theo t·ª´ng ƒëo·∫°n")
        for i, chunk_result in enumerate(results["translations"]):
            with st.expander(f"Chunk {i+1}", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**B·∫£n g·ªëc:**")
                    st.markdown(f'<div class="chunk-container">{chunk_result["original"]}</div>', unsafe_allow_html=True)
                
                with col2:
                    st.markdown("**B·∫£n d·ªãch:**")
                    st.markdown(f'<div class="translation-result">{chunk_result["translation"]}</div>', unsafe_allow_html=True)
                
                if chunk_result["keywords"]:
                    display_keywords(chunk_result["keywords"], "New Keywords in This Chunk")

    with tab3:
        st.markdown("### T·ª´ ƒëi·ªÉn T·ª´ kh√≥a")
        if results["global_keywords"]:
            # Search keywords
            search_term = st.text_input("T√¨m ki·∫øm t·ª´ kh√≥a:")
            
            filtered_keywords = results["global_keywords"]
            if search_term:
                filtered_keywords = {
                    k: v for k, v in results["global_keywords"].items() 
                    if search_term.lower() in k.lower() or search_term.lower() in v.lower()
                }
            
            # Display as table
            if filtered_keywords:
                df = pd.DataFrame(list(filtered_keywords.items()), columns=["English", "Vietnamese"])
                st.dataframe(df, use_container_width=True)
            else:
                st.info("Kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a n√†o ph√π h·ª£p v·ªõi t√¨m ki·∫øm")
        else:
            st.info("Ch∆∞a c√≥ t·ª´ kh√≥a n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t")

if __name__ == "__main__":
    main()